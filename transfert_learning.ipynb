{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "from keras.models import Sequential\n",
    "get_ipython().magic('matplotlib inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"/home/aims/Deep_learning_AnalyticsVidhya/transfer learning/Data/Train/train.csv\")\n",
    "test=pd.read_csv(\"/home/aims/Deep_learning_AnalyticsVidhya/transfer learning/Data/Test.csv\")\n",
    "train_path=\"/home/aims/Deep_learning_AnalyticsVidhya/transfer learning/Data/Train/Images/train/\"\n",
    "test_path=\"/home/aims/Deep_learning_AnalyticsVidhya/transfer learning/Data/Train/Images/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.misc import imresize\n",
    "# preparing the train dataset\n",
    "\n",
    "train_img=[]\n",
    "for i in range(len(train)):\n",
    "\n",
    "    temp_img=image.load_img(train_path+train['filename'][i],target_size=(224,224))\n",
    "\n",
    "    temp_img=image.img_to_array(temp_img)\n",
    "\n",
    "    train_img.append(temp_img)\n",
    "# Converting train image to array and applying mean substraction processing\n",
    "train_img = np.array(train_img)\n",
    "train_img = preprocess_input(train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# applying the same procedure with the dataset\n",
    "\n",
    "test_img = []\n",
    "for i in range(len(test)):\n",
    "    temp_img = image.load_img(test_path+test['filename'][i], target_size = (224,224))\n",
    "    temp_img = image.img_to_array(temp_img)\n",
    "    test_img.append(temp_img)\n",
    "    \n",
    "test_img = np.array(test_img)\n",
    "test_img = preprocess_input(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49000, 224, 224, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aims/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 48s 1us/step\n",
      "WARNING:tensorflow:From /home/aims/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loading VGG16 model weights\n",
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "# Extracting features from the train dataset using the VGG16 pre-trained model\n",
    "\n",
    "features_train=model.predict(train_img)\n",
    "# Extracting features from the train dataset using the VGG16 pre-trained model\n",
    "\n",
    "features_test=model.predict(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattening the layers to conform to MLP input\n",
    "\n",
    "train_x=features_train.reshape(49000,25088)\n",
    "# converting target variable to array\n",
    "\n",
    "train_y=np.asarray(train['label'])\n",
    "# performing one-hot encoding for the target variable\n",
    "\n",
    "train_y=pd.get_dummies(train_y)\n",
    "train_y=np.array(train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating training and validation set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, Y_train, Y_valid=train_test_split(train_x,train_y,test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a mlp model\n",
    "from keras.layers import Dense, Activation\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Dense(1000, input_dim=25088, activation='relu',kernel_initializer='uniform'))\n",
    "keras.layers.core.Dropout(0.3, noise_shape=None, seed=None)\n",
    "\n",
    "model.add(Dense(500,input_dim=1000,activation='sigmoid'))\n",
    "keras.layers.core.Dropout(0.4, noise_shape=None, seed=None)\n",
    "\n",
    "model.add(Dense(150,input_dim=500,activation='sigmoid'))\n",
    "keras.layers.core.Dropout(0.2, noise_shape=None, seed=None)\n",
    "\n",
    "model.add(Dense(units=10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model \n",
    "\n",
    "model.fit(X_train, Y_train, epochs=20, batch_size=128,validation_data=(X_valid,Y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for freezing the weights of first few layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "#from scipy.misc import imread\n",
    "get_ipython().magic('matplotlib inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "train=pd.read_csv(\"/home/aims/Deep_learning_AnalyticsVidhya/transfer learning/Data/Train/train.csv\")\n",
    "test=pd.read_csv(\"/home/aims/Deep_learning_AnalyticsVidhya/transfer learning/Data/Test.csv\")\n",
    "train_path=\"/home/aims/Deep_learning_AnalyticsVidhya/transfer learning/Data/Train/Images/train/\"\n",
    "test_path=\"/home/aims/Deep_learning_AnalyticsVidhya/transfer learning/Data/Train/Images/test/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img=[]\n",
    "for i in range(len(train)):\n",
    "    temp_img=image.load_img(train_path+train['filename'][i],target_size=(224,224))\n",
    "    temp_img=image.img_to_array(temp_img)\n",
    "    train_img.append(temp_img)\n",
    "\n",
    "train_img=np.array(train_img) \n",
    "train_img=preprocess_input(train_img)\n",
    "\n",
    "test_img=[]\n",
    "for i in range(len(test)):\n",
    "    temp_img=image.load_img(test_path+test['filename'][i],target_size=(224,224))\n",
    "    temp_img=image.img_to_array(temp_img)\n",
    "    test_img.append(temp_img)\n",
    "\n",
    "test_img=np.array(test_img) \n",
    "test_img=preprocess_input(test_img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "def vgg16_model(img_rows, img_cols, channel=1, num_classes=None):\n",
    "\n",
    "    model = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "    model.layers.pop()\n",
    "\n",
    "    model.outputs = [model.layers[-1].output]\n",
    "\n",
    "    model.layers[-1].outbound_nodes = []\n",
    "\n",
    "    x=Dense(num_classes, activation='softmax')(model.output)\n",
    "\n",
    "    model=Model(model.input,x)\n",
    "    #To set the first 8 layers to non-trainable (weights will not be updated)\n",
    "\n",
    "    for layer in model.layers[:8]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Learning rate is changed to 0.001\n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y=np.asarray(train['label'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "train_y = le.fit_transform(train_y)\n",
    "\n",
    "train_y=to_categorical(train_y)\n",
    "\n",
    "train_y=np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, Y_train, Y_valid=train_test_split(train_img,train_y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to fine-tune on 3000 samples from Cifar10\n",
    "\n",
    "img_rows, img_cols = 224, 224 # Resolution of inputs\n",
    "channel = 3\n",
    "num_classes = 10 \n",
    "batch_size = 16 \n",
    "nb_epoch = 10\n",
    "\n",
    "# Load our model\n",
    "model = vgg16_model(img_rows, img_cols, channel, num_classes)\n",
    "\n",
    "model.summary()\n",
    "# Start Fine-tuning\n",
    "model.fit(X_train, Y_train,batch_size=batch_size,epochs=nb_epoch,shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions_valid = model.predict(X_valid, batch_size=batch_size, verbose=1)\n",
    "\n",
    "# Cross-entropy loss score\n",
    "score = log_loss(Y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
